**Robots.txt** - это текстовый файл, который содержит параметры индексирования сайта для роботов поисковых систем.
Директивы:  
1. _User-agent*_ - Указывает на робота, для которого действуют перечисленные в robots.txt правила. **Обязательная директива.**
2. _Disallow_ - Запрещает индексирование разделов или отдельных страниц сайта.
3. _Sitemap_ - Указывает путь к файлу Sitemap, который размещен на сайте.
4. _Clean-param_ - Указывает роботу, что URL страницы содержит параметры (например, UTM-метки), которые не нужно учитывать при индексировании.
5. _Allow_ - Разрешает индексирование разделов или отдельных страниц сайта.
6. _Crawl-delay_ - Задает роботу минимальный период времени (в секундах) между окончанием загрузки одной страницы и началом загрузки следующей.


**!Роботы других поисковых систем и сервисов могут иначе интерпретировать директивы.**
- Роботы других поисковых систем и сервисов могут иначе интерпретировать директивы.
Страницы, которые необходимо закрывать для поиска с помощью robots.txt.

- Из индексации сайта, продающего товары онлайн, следует исключать страницы с размещенными формами заказа. 
   1. Страницы с фильтрами для товаров в каталогах.
   2. Не должна индексироваться корзина покупок.
   3. URL, содержащие внутренний поиск, которые могут создавать дубли.
   4. web-документы, содержащие формы регистрации на сайте, поскольку на них вводится личная информация.  
- Иногда эти страницы можно оставить открытыми для индексации. Это необходимо сделать, если их коды содержат keywords.
